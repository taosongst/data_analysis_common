{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29ad295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datatable as dt\n",
    "# import gresearch_crypto\n",
    "# from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "# data_path = '../input/g-research-crypto-forecasting/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.core.common.SettingWithCopyWarning)\n",
    "    \n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['figure.figsize'] = [14, 8]  # width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d973e9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423681e+07</td>\n",
       "      <td>2.423680e+07</td>\n",
       "      <td>2.348647e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.577120e+09</td>\n",
       "      <td>6.292544e+00</td>\n",
       "      <td>2.864593e+02</td>\n",
       "      <td>1.432640e+03</td>\n",
       "      <td>1.436350e+03</td>\n",
       "      <td>1.429568e+03</td>\n",
       "      <td>1.432640e+03</td>\n",
       "      <td>2.868530e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.121752e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.323350e+07</td>\n",
       "      <td>4.091861e+00</td>\n",
       "      <td>8.673982e+02</td>\n",
       "      <td>6.029605e+03</td>\n",
       "      <td>6.039482e+03</td>\n",
       "      <td>6.020261e+03</td>\n",
       "      <td>6.029611e+03</td>\n",
       "      <td>2.433935e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.679042e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.170400e-03</td>\n",
       "      <td>1.195000e-03</td>\n",
       "      <td>2.000000e-04</td>\n",
       "      <td>1.171400e-03</td>\n",
       "      <td>-3.662812e-01</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-5.093509e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.549011e+09</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>2.676500e-01</td>\n",
       "      <td>2.681600e-01</td>\n",
       "      <td>2.669000e-01</td>\n",
       "      <td>2.676483e-01</td>\n",
       "      <td>1.410725e+02</td>\n",
       "      <td>2.676368e-01</td>\n",
       "      <td>-1.694354e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.578372e+09</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>1.428860e+01</td>\n",
       "      <td>1.431250e+01</td>\n",
       "      <td>1.426300e+01</td>\n",
       "      <td>1.428920e+01</td>\n",
       "      <td>1.295415e+03</td>\n",
       "      <td>1.428769e+01</td>\n",
       "      <td>-4.289844e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.606198e+09</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>2.210000e+02</td>\n",
       "      <td>2.288743e+02</td>\n",
       "      <td>2.293000e+02</td>\n",
       "      <td>2.284200e+02</td>\n",
       "      <td>2.288729e+02</td>\n",
       "      <td>2.729764e+04</td>\n",
       "      <td>2.288728e+02</td>\n",
       "      <td>1.601520e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.632182e+09</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>1.650160e+05</td>\n",
       "      <td>6.480594e+04</td>\n",
       "      <td>6.490000e+04</td>\n",
       "      <td>6.467053e+04</td>\n",
       "      <td>6.480854e+04</td>\n",
       "      <td>7.597554e+08</td>\n",
       "      <td>inf</td>\n",
       "      <td>9.641699e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp      Asset_ID         Count          Open          High  \\\n",
       "count  2.423681e+07  2.423681e+07  2.423681e+07  2.423681e+07  2.423681e+07   \n",
       "mean   1.577120e+09  6.292544e+00  2.864593e+02  1.432640e+03  1.436350e+03   \n",
       "std    3.323350e+07  4.091861e+00  8.673982e+02  6.029605e+03  6.039482e+03   \n",
       "min    1.514765e+09  0.000000e+00  1.000000e+00  1.170400e-03  1.195000e-03   \n",
       "25%    1.549011e+09  3.000000e+00  1.900000e+01  2.676500e-01  2.681600e-01   \n",
       "50%    1.578372e+09  6.000000e+00  6.400000e+01  1.428860e+01  1.431250e+01   \n",
       "75%    1.606198e+09  9.000000e+00  2.210000e+02  2.288743e+02  2.293000e+02   \n",
       "max    1.632182e+09  1.300000e+01  1.650160e+05  6.480594e+04  6.490000e+04   \n",
       "\n",
       "                Low         Close        Volume          VWAP        Target  \n",
       "count  2.423681e+07  2.423681e+07  2.423681e+07  2.423680e+07  2.348647e+07  \n",
       "mean   1.429568e+03  1.432640e+03  2.868530e+05           NaN  7.121752e-06  \n",
       "std    6.020261e+03  6.029611e+03  2.433935e+06           NaN  5.679042e-03  \n",
       "min    2.000000e-04  1.171400e-03 -3.662812e-01          -inf -5.093509e-01  \n",
       "25%    2.669000e-01  2.676483e-01  1.410725e+02  2.676368e-01 -1.694354e-03  \n",
       "50%    1.426300e+01  1.428920e+01  1.295415e+03  1.428769e+01 -4.289844e-05  \n",
       "75%    2.284200e+02  2.288729e+02  2.729764e+04  2.288728e+02  1.601520e-03  \n",
       "max    6.467053e+04  6.480854e+04  7.597554e+08           inf  9.641699e-01  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.describe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35488e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Asset_ID', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9bba7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>78.38</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>-0.014399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1514764920</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5145</td>\n",
       "      <td>8.5145</td>\n",
       "      <td>71.39</td>\n",
       "      <td>8.520215</td>\n",
       "      <td>-0.015875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1514764980</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.5065</td>\n",
       "      <td>8.5299</td>\n",
       "      <td>8.4848</td>\n",
       "      <td>8.4848</td>\n",
       "      <td>1546.82</td>\n",
       "      <td>8.501394</td>\n",
       "      <td>-0.015410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp  Asset_ID  Count    Open    High     Low   Close   Volume  \\\n",
       "1   1514764860         0    5.0  8.5300  8.5300  8.5300  8.5300    78.38   \n",
       "9   1514764920         0    7.0  8.5300  8.5300  8.5145  8.5145    71.39   \n",
       "17  1514764980         0   45.0  8.5065  8.5299  8.4848  8.4848  1546.82   \n",
       "\n",
       "        VWAP    Target  \n",
       "1   8.530000 -0.014399  \n",
       "9   8.520215 -0.015875  \n",
       "17  8.501394 -0.015410  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ee89a",
   "metadata": {},
   "source": [
    "#### parse timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e950ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "      <th>time_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>78.38</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>-0.014399</td>\n",
       "      <td>2018-01-01 00:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1514764920</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5145</td>\n",
       "      <td>8.5145</td>\n",
       "      <td>71.39</td>\n",
       "      <td>8.520215</td>\n",
       "      <td>-0.015875</td>\n",
       "      <td>2018-01-01 00:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1514764980</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.5065</td>\n",
       "      <td>8.5299</td>\n",
       "      <td>8.4848</td>\n",
       "      <td>8.4848</td>\n",
       "      <td>1546.82</td>\n",
       "      <td>8.501394</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>2018-01-01 00:03:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     timestamp  Asset_ID  Count    Open    High     Low   Close   Volume  \\\n",
       "1   1514764860         0    5.0  8.5300  8.5300  8.5300  8.5300    78.38   \n",
       "9   1514764920         0    7.0  8.5300  8.5300  8.5145  8.5145    71.39   \n",
       "17  1514764980         0   45.0  8.5065  8.5299  8.4848  8.4848  1546.82   \n",
       "\n",
       "        VWAP    Target         time_parsed  \n",
       "1   8.530000 -0.014399 2018-01-01 00:01:00  \n",
       "9   8.520215 -0.015875 2018-01-01 00:02:00  \n",
       "17  8.501394 -0.015410 2018-01-01 00:03:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_parsed'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b81a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df['time_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "763046e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "      <th>time_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_parsed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:01:00</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>78.38</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>-0.014399</td>\n",
       "      <td>2018-01-01 00:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:02:00</th>\n",
       "      <td>1514764920</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5300</td>\n",
       "      <td>8.5145</td>\n",
       "      <td>8.5145</td>\n",
       "      <td>71.39</td>\n",
       "      <td>8.520215</td>\n",
       "      <td>-0.015875</td>\n",
       "      <td>2018-01-01 00:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:03:00</th>\n",
       "      <td>1514764980</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.5065</td>\n",
       "      <td>8.5299</td>\n",
       "      <td>8.4848</td>\n",
       "      <td>8.4848</td>\n",
       "      <td>1546.82</td>\n",
       "      <td>8.501394</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>2018-01-01 00:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:04:00</th>\n",
       "      <td>1514765040</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.5009</td>\n",
       "      <td>8.5066</td>\n",
       "      <td>8.4744</td>\n",
       "      <td>8.5009</td>\n",
       "      <td>125.80</td>\n",
       "      <td>8.479810</td>\n",
       "      <td>-0.012524</td>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>1514765100</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.5007</td>\n",
       "      <td>8.5007</td>\n",
       "      <td>8.4560</td>\n",
       "      <td>8.4560</td>\n",
       "      <td>125.01</td>\n",
       "      <td>8.458435</td>\n",
       "      <td>-0.005940</td>\n",
       "      <td>2018-01-01 00:05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp  Asset_ID  Count    Open    High     Low  \\\n",
       "time_parsed                                                                \n",
       "2018-01-01 00:01:00  1514764860         0    5.0  8.5300  8.5300  8.5300   \n",
       "2018-01-01 00:02:00  1514764920         0    7.0  8.5300  8.5300  8.5145   \n",
       "2018-01-01 00:03:00  1514764980         0   45.0  8.5065  8.5299  8.4848   \n",
       "2018-01-01 00:04:00  1514765040         0   14.0  8.5009  8.5066  8.4744   \n",
       "2018-01-01 00:05:00  1514765100         0    5.0  8.5007  8.5007  8.4560   \n",
       "\n",
       "                      Close   Volume      VWAP    Target         time_parsed  \n",
       "time_parsed                                                                   \n",
       "2018-01-01 00:01:00  8.5300    78.38  8.530000 -0.014399 2018-01-01 00:01:00  \n",
       "2018-01-01 00:02:00  8.5145    71.39  8.520215 -0.015875 2018-01-01 00:02:00  \n",
       "2018-01-01 00:03:00  8.4848  1546.82  8.501394 -0.015410 2018-01-01 00:03:00  \n",
       "2018-01-01 00:04:00  8.5009   125.80  8.479810 -0.012524 2018-01-01 00:04:00  \n",
       "2018-01-01 00:05:00  8.4560   125.01  8.458435 -0.005940 2018-01-01 00:05:00  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c950dd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp           0\n",
       "Asset_ID            0\n",
       "Count               0\n",
       "Open                0\n",
       "High                0\n",
       "Low                 0\n",
       "Close               0\n",
       "Volume              0\n",
       "VWAP                9\n",
       "Target         750338\n",
       "time_parsed         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99ba8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_parsed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:01:00</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>329.0900</td>\n",
       "      <td>329.880</td>\n",
       "      <td>329.09</td>\n",
       "      <td>329.46</td>\n",
       "      <td>6.635710</td>\n",
       "      <td>329.454118</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:06:00</th>\n",
       "      <td>1514765160</td>\n",
       "      <td>7</td>\n",
       "      <td>76.0</td>\n",
       "      <td>26.4255</td>\n",
       "      <td>27.073</td>\n",
       "      <td>25.55</td>\n",
       "      <td>26.40</td>\n",
       "      <td>1857.936529</td>\n",
       "      <td>26.352542</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:12:00</th>\n",
       "      <td>1514765520</td>\n",
       "      <td>11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>327.6700</td>\n",
       "      <td>327.680</td>\n",
       "      <td>326.77</td>\n",
       "      <td>326.77</td>\n",
       "      <td>17.901633</td>\n",
       "      <td>326.990493</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp  Asset_ID  Count      Open     High     Low  \\\n",
       "time_parsed                                                                   \n",
       "2018-01-01 00:01:00  1514764860        11    7.0  329.0900  329.880  329.09   \n",
       "2018-01-01 00:06:00  1514765160         7   76.0   26.4255   27.073   25.55   \n",
       "2018-01-01 00:12:00  1514765520        11    8.0  327.6700  327.680  326.77   \n",
       "\n",
       "                      Close       Volume        VWAP  Target  \n",
       "time_parsed                                                   \n",
       "2018-01-01 00:01:00  329.46     6.635710  329.454118     NaN  \n",
       "2018-01-01 00:06:00   26.40  1857.936529   26.352542     NaN  \n",
       "2018-01-01 00:12:00  326.77    17.901633  326.990493     NaN  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Target'].isna()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bd4921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['time_parsed'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "442f18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(['time_parsed','Asset_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffd86f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_approximate(x):\n",
    "    return (x-1)-1/2*(x-1)*(x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b2cac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.assign(target_computed = df.VWAP.apply(log_approximate).groupby(df.Asset_ID).diff(-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486e2e9",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff89d0d",
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfa9bd",
   "metadata": {},
   "source": [
    "# Reduced Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "516b8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0dc87a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2034.03 MB\n",
      "Memory usage after optimization is: 924.56 MB\n",
      "Decreased by 54.5%\n"
     ]
    }
   ],
   "source": [
    "df_red = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc7512",
   "metadata": {},
   "source": [
    "# Configure the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4365609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from random import choices\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "\n",
    "#simply think of it as a matrix multiplication function. In reality depth = number of features, seq_len_q = seq_len_k = seq_len_v\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    for a input of (batch_size, seq_length, features), with parameters = (heads, depth). Let d_model = heads * depth\n",
    "    first use three (features , (depth * heads)) to map it into three (batch, seq_length, d_model)\n",
    "    starting from the second layer, use (d_model, d_model) to do the same thing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    refer : https://github.com/google-research/bert/blob/bee6030e31e42a9394ac567da170a89a98d2062f/modeling.py#L264\n",
    "    Args:\n",
    "        x: float Tensor to perform activation.\n",
    "    Returns:\n",
    "        `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh(\n",
    "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation = gelu),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate = 0.1):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.rate = rate\n",
    "\n",
    "        self.embedding = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(self.rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        layer.get_config() can return config of this layer, and then a new layer can be \n",
    "        initialized using Layer.from_config().\n",
    "        \"\"\"\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dff': self.dff,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training, mask = None):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.dropout(x, training = training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0839095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model(input_shape, num_columns, num_labels, weights = None):\n",
    "    d_model = 5\n",
    "    num_heads = 10\n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = TransformerEncoder(2, d_model*num_heads, num_heads, 64)(inp)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    out = tf.keras.layers.Dense(num_labels, activation = 'linear')(x)\n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    model.compile(optimizer = 'adam', \n",
    "                  loss = tf.keras.losses.MeanSquaredError(), metrics = [tf.keras.metrics.MeanSquaredError()])   \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea851d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model = single_model((11,7),5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f501419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 11, 7)]           0         \n",
      "_________________________________________________________________\n",
      "transformer_encoder_3 (Trans (None, 11, 50)            34228     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 550)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 551       \n",
      "=================================================================\n",
      "Total params: 34,779\n",
      "Trainable params: 34,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "single_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0c2686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp, num_columns, num_labels = 1, weights = None):\n",
    "    inp = tf.keras.layers.Input(shape = (num_columns, ))\n",
    "    x = tf.keras.layers.Reshape((1, num_columns))(inp)\n",
    "    \n",
    "    num_layers = hp.Int(f'num_layers', 1, 5)\n",
    "    d_model = hp.Int('d_model_mult', 1, 50)\n",
    "    num_heads = hp.Int('num_heads', 1, 12)\n",
    "    dff = hp.Int('dff', 64, 256)\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.0, 0.5)\n",
    "\n",
    "    x = TransformerEncoder(num_layers, d_model * num_heads, num_heads, dff, dropout_rate)(x)[:, 0, :]\n",
    "\n",
    "    out = tf.keras.layers.Dense(num_labels, activation = 'linear')(x)\n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = hp.Float('lr', 0.00001, 0.1, default = 0.001)), \n",
    "                  loss = tf.keras.losses.MeanSquaredError(), metrics = [nn_comp_metric(weights)] if weights is not None else [])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3afe01",
   "metadata": {},
   "source": [
    "# Keras Tuner Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99606b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(\n",
    "      hp.Choice('units', [8, 16, 32]),\n",
    "      activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda21859",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
